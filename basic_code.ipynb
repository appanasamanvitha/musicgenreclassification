import pandas as pd
import numpy as np
df = pd.read_csv('C:\\ml model dep\\data.csv')
df = df.drop(['beats'], axis=1)
df.head()

df['class_name'].unique()
df['class_name'] = df['class_name'].astype('category')
lookup_genre_name = dict(zip(df.class_label.unique(), df.label.unique()))   
lookup_genre_name
df['class_name'].unique()

cols = list(df.columns)
cols.remove('label')
cols.remove('class_label')
cols.remove('class_name')
%matplotlib notebook
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
X = df.iloc[:,1:28]
y = df['class_label']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)

from sklearn.ensemble import RandomForestClassifier
%matplotlib notebook
clf = RandomForestClassifier(random_state=0, n_jobs=-1).fit(X_train_scaled, y_train)
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]
names = [X.columns.values[i] for i in indices]
plt.figure()
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), names, rotation=90)
plt.show()
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0).fit(X_train_scaled, y_train)
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]
names = [X.columns.values[i] for i in indices]
plt.figure()
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), names, rotation=90)
plt.show()
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 13)
knn.fit(X_train_scaled, y_train)
knn.score(X_test_scaled, y_test)
import librosa
import numpy as np

def getmetadata(filename):
    y, sr = librosa.load(filename)
    
    
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]


    chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))

    
    rmse = np.mean(librosa.feature.rms(y=y))

    
    spec_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))

   
    spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))

  
    spec_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))

   
    zero_crossing = np.mean(librosa.feature.zero_crossing_rate(y))

   
    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20), axis=1)

    
    metadata_dict = {
        'tempo': tempo,
        'chroma_stft': chroma_stft,
        'rmse': rmse,
        'spectral_centroid': spec_centroid,
        'spectral_bandwidth': spec_bw,
        'rolloff': spec_rolloff,
        'zero_crossing_rates': zero_crossing
    }

    for i in range(1, 21):
        metadata_dict[f'mfcc{i}'] = mfcc[i-1]

    return list(metadata_dict.values())
from Metadata import getmetadata
a = getmetadata("C:\\ml model dep\\archive\\Data\\genres_original\\rock\\rock.00026.wav")
from sklearn.svm import SVC
clf = SVC(kernel = 'linear', C=10).fit(X_train_scaled, y_train)
clf.score(X_test_scaled, y_test)
d1 =np.array(a)
data1 = scaler.transform([d1])
genre_prediction = clf.predict(data1)
print(lookup_genre_name[genre_prediction[0]])
import pickle
pick1 = {
    'norma':scaler,
    'svmp':knn,
    'lgn':lookup_genre_name
}
pickle.dump( pick1, open( 'models' + ".p", "wb" ) )
import pickle
filename = 'trainingmodel.sav'
pickle.dump(clf,open(filename,"wb"))
loaded_model = pickle.load(open('trainingmodel.sav','rb'))
d1 =np.array(a)
data1 = scaler.transform([d1])
genre_prediction = clf.predict(data1)
print(lookup_genre_name[genre_prediction[0]])
